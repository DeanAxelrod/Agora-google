<!DOCTYPE html>
<html>
<head>
  <title>Agora Voice Recorder with Classification</title>
  <script src="https://cdn.agora.io/sdk/release/AgoraRTCSDK-3.6.9.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands"></script>
  <style>
    body { font-family: Arial, sans-serif; }
    button { margin: 5px; }
    #debug { 
      margin-top: 20px;
      padding: 10px;
      border: 1px solid #ccc;
      max-height: 300px;
      overflow-y: auto;
    }
  </style>
</head>
<body>
  <h1>Agora Voice Recorder with Classification</h1>
  <button id="startBtn">Start Recording</button>
  <button id="stopBtn" disabled>Stop Recording</button>
  <p id="recordingStatus">Recording Status: Idle</p>
  <p id="classificationResult">Classification Result: N/A</p>
  <div id="debug"></div>

  <script>
    const appId = "25336bcd01664c70870804ba0b7e1b30";
    const channelName = "test_channel";
    const modelURL = "https://deanaxelrod.github.io/Google-TM/model.json";
    const classLabels = ["Noise", "Sad Dog", "Excited Dog (panting)", "Disappointed Dog (sigh)", 
                        "Threatening dog (Growl)", "Attention seeking Dog (whimpering)", 
                        "Excited Territorial Dog (barking)"];

    const client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
    let localStream;
    let mediaRecorder;
    let recordedChunks = [];
    let model;

    function debugLog(message, data = null) {
        const text = data ? `${message} ${JSON.stringify(data)}` : message;
        console.log(text);
        const debugDiv = document.getElementById('debug');
        const logEntry = document.createElement('p');
        logEntry.textContent = text;
        debugDiv.appendChild(logEntry);
        debugDiv.scrollTop = debugDiv.scrollHeight;
    }

async function classifyAudio(audioBlob) {
  try {
    // Create an AudioContext (ensure this is done after a user gesture; your button click should suffice)
    const audioContext = new AudioContext();
    const arrayBuffer = await audioBlob.arrayBuffer();
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    console.log("Audio buffer duration (s):", audioBuffer.duration);
    
    // Use the provided convertToSpectrogram function (which does not use AnalyserNode)
    const spectrogram = convertToSpectrogram(audioBuffer);
    console.log("Spectrogram tensor shape:", spectrogram.shape);
    
    // Predict and print raw prediction tensor for debugging
    const prediction = model.predict(spectrogram);
    console.log("Raw prediction tensor:");
    prediction.print();

    // Get the predicted class index
    const resultIndex = prediction.argMax(1).dataSync()[0];
    console.log("Classification index:", resultIndex);

    // Map the index to a human-readable label
    let resultLabel = classLabels[resultIndex] || ("Index " + resultIndex);
    console.log("Mapped classification label:", resultLabel);
    return resultLabel;
  } catch (error) {
    console.error("Error during classification:", error);
    return null;
  }
}


    async function classifyAudio(audioBlob) {
        try {
            debugLog("Starting classification...");
            const audioContext = new AudioContext();
            const arrayBuffer = await audioBlob.arrayBuffer();
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            
            debugLog("Audio properties:", {
                duration: audioBuffer.duration,
                sampleRate: audioBuffer.sampleRate,
                numberOfChannels: audioBuffer.numberOfChannels
            });

            const spectrogram = await createSpectrogram(audioBuffer);
            
            debugLog("Running prediction...");
            const prediction = await model.predict(spectrogram).data();
            
            classLabels.forEach((label, idx) => {
                debugLog(`${label}: ${(prediction[idx] * 100).toFixed(2)}%`);
            });
            
            const resultIndex = prediction.indexOf(Math.max(...prediction));
            return classLabels[resultIndex];
        } catch (error) {
            debugLog("Classification error:", {
                message: error.message,
                stack: error.stack
            });
            return null;
        }
    }

    async function loadModel() {
        try {
            model = await tf.loadLayersModel(modelURL);
            debugLog("Model loaded successfully");
            const inputShape = model.inputs[0].shape;
            debugLog("Model expects input shape:", inputShape);
            model.summary();
            return true;
        } catch (error) {
            debugLog("Error loading model:", error);
            return false;
        }
    }

    // Fixed Start Recording Function
    function startRecording() {
        try {
            debugLog("Attempting to start recording...");
            client.join(null, channelName, null, (uid) => {
                debugLog("User joined with UID:", uid);
                localStream = AgoraRTC.createStream({
                    audio: true,
                    video: false
                });

                localStream.init(() => {
                    debugLog("Local stream initialized");
                    client.publish(localStream, (err) => {
                        if(err) {
                            debugLog("Publish error:", err);
                            return;
                        }
                        debugLog("Stream published successfully");
                    });

                    document.getElementById("recordingStatus").innerText = "Recording Status: Recording...";
                    document.getElementById("startBtn").disabled = true;
                    document.getElementById("stopBtn").disabled = false;

                    try {
                        const mediaStream = localStream.stream || new MediaStream([localStream.getAudioTrack()]);
                        mediaRecorder = new MediaRecorder(mediaStream);
                        recordedChunks = [];

                        mediaRecorder.ondataavailable = (event) => {
                            if (event.data.size > 0) {
                                recordedChunks.push(event.data);
                            }
                        };

                        mediaRecorder.start();
                        debugLog("MediaRecorder started successfully");
                    } catch (e) {
                        debugLog("MediaRecorder error:", e);
                    }
                }, (error) => {
                    debugLog("Local stream initialization error:", error);
                });
            }, (error) => {
                debugLog("Join channel error:", error);
            });
        } catch (error) {
            debugLog("Error in startRecording:", error);
        }
    }

    // Fixed Stop Recording Function
    function stopRecording() {
        try {
            debugLog("Stopping recording...");
            
            if (mediaRecorder && mediaRecorder.state !== "inactive") {
                mediaRecorder.stop();
                mediaRecorder.onstop = async () => {
                    try {
                        const audioBlob = new Blob(recordedChunks, { type: 'audio/webm' });
                        recordedChunks = [];
                        
                        debugLog("Processing recorded audio...");
                        const classificationLabel = await classifyAudio(audioBlob);
                        
                        if (classificationLabel !== null) {
                            document.getElementById("classificationResult").innerText = 
                                "Classification Result: " + classificationLabel;
                        } else {
                            document.getElementById("classificationResult").innerText = 
                                "Classification Result: Failed";
                        }
                    } catch (error) {
                        debugLog("Error processing recording:", error);
                    }
                };
            }

            client.unpublish(localStream, (err) => {
                if (err) {
                    debugLog("Unpublish error:", err);
                }
            });

            client.leave(() => {
                debugLog("Client left channel");
                localStream.close();
                document.getElementById("recordingStatus").innerText = "Recording Status: Idle";
                document.getElementById("startBtn").disabled = false;
                document.getElementById("stopBtn").disabled = true;
            }, (error) => {
                debugLog("Leave channel error:", error);
            });
        } catch (error) {
            debugLog("Error in stopRecording:", error);
        }
    }

    // Initialize
    client.init(appId, async () => {
        debugLog("Agora initialized successfully.");
        document.getElementById("recordingStatus").innerText = "Recording Status: Ready";
        await loadModel();
    }, (error) => {
        debugLog("Agora initialization error:", error);
    });

    // Event Listeners
    document.getElementById("startBtn").addEventListener("click", startRecording);
    document.getElementById("stopBtn").addEventListener("click", stopRecording);
  </script>
</body>
</html>
