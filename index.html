<!DOCTYPE html>
<html>
<head>
  <title>Agora Voice Recorder</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands"></script>
  <script src="https://cdn.agora.io/sdk/release/AgoraRTCSDK-3.6.9.js"></script>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      margin: 0; 
      padding: 20px; 
      box-sizing: border-box; 
      text-align: center;
    }
    .container { 
      max-width: 600px;
      background: none;
      margin: 0 auto; 
      padding: 0;
    }
    /* Stack the buttons vertically */
    .button-container { 
      margin: 20px 0; 
      display: flex; 
      flex-direction: column;
      gap: 10px;
      align-items: center;
       background: none;
    }
    /* Make the buttons smaller */
    button {
      margin: 5px; 
      padding: 8px 16px;        /* Reduced padding for a smaller button */
      cursor: pointer;
      background-color: #4CAF50;  /* Green background for the start button */
      color: white;               /* White text */
      border: none;
      border-radius: 4px; 
      font-size: 14px;            /* Smaller font size */
      min-width: 150px;           /* Reduced minimum width */
      transition: all 0.3s ease;
    }
    button:disabled { 
      background-color: #cccccc; 
      cursor: not-allowed; 
    }
    /* Stop button defaults */
    .stop-button { 
      background-color: #cccccc;  /* Grey background by default */
    }
    /* When recording, the stop button turns red */
    .stop-button.recording { 
      background-color: #f44336 !important;  
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="button-container">
      <button id="startBtn">Start Recording</button>
      <button id="stopBtn" class="stop-button" disabled>Stop Recording</button>
    </div>
  </div>

  <script>
    const appId = "25336bcd01664c70870804ba0b7e1b30";
    const modelURL = "https://deanaxelrod.github.io/Google-TM/model.json";
    const classLabels = ["Noise", "Sad Dog", "Excited Dog", "Disappointed Dog", "Growl", "Whimper", "Bark"];

    let mediaRecorder = null;
    let recordedChunks = [];
    let model = null;

    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');

    // Update the buttons' state and styling.
    function setRecordingState(recording) {
      startBtn.disabled = recording;
      stopBtn.disabled = !recording;
      if (recording) {
        stopBtn.classList.add('recording');
      } else {
        stopBtn.classList.remove('recording');
      }
    }

    async function checkMicrophonePermission() {
      try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
          throw new Error('getUserMedia not supported');
        }
        if (navigator.permissions) {
          const permissionStatus = await navigator.permissions.query({ name: 'microphone' });
          if (permissionStatus.state === 'denied') {
            throw new Error('Microphone access denied');
          }
        }
        await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        return true;
      } catch (error) {
        startBtn.disabled = true;
        console.error("Microphone Permission Error:", error.message);
        throw error;
      }
    }

    async function createSpectrogram(audioBuffer) {
      const audioData = audioBuffer.getChannelData(0);
      const fftSize = 512;
      const hopSize = fftSize / 2;
      const numFrames = Math.floor((audioData.length - fftSize) / hopSize);
      const spectrogramData = [];
      for (let i = 0; i < numFrames; i++) {
        const startIdx = i * hopSize;
        const frameData = audioData.slice(startIdx, startIdx + fftSize);
        if (frameData.length === fftSize) {
          const tensorFrame = tf.tensor1d(frameData);
          const windowedFrame = tensorFrame.mul(tf.signal.hammingWindow(fftSize));
          const fft = tf.spectral.rfft(windowedFrame);
          const magnitude = tf.abs(fft);
          spectrogramData.push(magnitude.arraySync());
          fft.dispose();
          tensorFrame.dispose();
        }
      }
      const spectrogramTensor = tf.tensor2d(spectrogramData);
      const targetHeight = 43;
      const targetWidth = 232;
      const spectrogramWithChannel = spectrogramTensor.reshape([1, spectrogramTensor.shape[0], spectrogramTensor.shape[1], 1]);
      const resizedSpectrogram = tf.image.resizeBilinear(spectrogramWithChannel, [targetHeight, targetWidth]);
      spectrogramTensor.dispose();
      const normalizedSpectrogram = resizedSpectrogram.div(tf.max(resizedSpectrogram));
      return normalizedSpectrogram.reshape([1, targetHeight, targetWidth, 1]);
    }

    async function classifyAudio(audioBlob) {
      const audioContext = new AudioContext();
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      const spectrogram = await createSpectrogram(audioBuffer);
      const prediction = await model.predict(spectrogram).data();
      spectrogram.dispose();
      const resultIndex = prediction.indexOf(Math.max(...prediction));
      const classificationLabel = classLabels[resultIndex];
      const simpleMessage = {
        type: 'audioClassification',
        classification: classificationLabel
      };
      try {
        if (window.ReactNativeWebView) {
          window.ReactNativeWebView.postMessage(JSON.stringify(simpleMessage));
        } else if (window.webkit && window.webkit.messageHandlers) {
          window.webkit.messageHandlers.message.postMessage(simpleMessage);
        } else {
          window.parent.postMessage(simpleMessage, '*');
        }
      } catch (sendError) {
        console.error("Error sending message:", sendError.message);
      }
      return classificationLabel;
    }

    async function startRecording() {
      try {
        await checkMicrophonePermission();
        setRecordingState(true);
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        mediaRecorder = new MediaRecorder(stream);
        recordedChunks = [];
        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            recordedChunks.push(event.data);
          }
        };
        mediaRecorder.start();
      } catch (error) {
        console.error("Error starting recording:", error.message);
      }
    }

    async function stopRecording() {
      try {
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
          await new Promise(resolve => {
            mediaRecorder.onstop = resolve;
            mediaRecorder.stop();
          });
        }
        if (recordedChunks.length > 0) {
          const audioBlob = new Blob(recordedChunks, { type: 'audio/webm' });
          try {
            await classifyAudio(audioBlob);
          } catch (error) {
            console.error("Classification failed:", error.message);
            window.parent.postMessage({
              type: 'error',
              message: error.message
            }, '*');
          }
        }
        setRecordingState(false);
      } catch (error) {
        console.error("Error stopping recording:", error.message);
        setRecordingState(false);
      }
    }

    async function initialize() {
      try {
        await tf.setBackend('cpu');
        model = await tf.loadLayersModel(modelURL);
        startBtn.disabled = false;
      } catch (error) {
        console.error("Initialization error:", error.message);
        startBtn.disabled = true;
      }
    }

    startBtn.addEventListener('click', async () => {
      try {
        await checkMicrophonePermission();
        await startRecording();
      } catch (error) {
        console.error("Recording start error:", error.message);
      }
    });

    stopBtn.addEventListener('click', stopRecording);

    window.addEventListener('beforeunload', () => {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }
    });

    initialize();
  </script>
</body>
</html>
