<!DOCTYPE html>
<html>
<head>
    <title>Agora Voice Recorder</title>
    <script src="https://cdn.agora.io/sdk/release/AgoraRTCSDK-3.6.9.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands"></script>
</head>
<body>
    <h1>Agora Voice Recorder</h1>
    <button id="startBtn">Start Recording</button>
    <button id="stopBtn" disabled>Stop Recording</button>
    <p id="status">Status: Idle</p>

    <script>
        const appId = "25336bcd01664c70870804ba0b7e1b30";
        const client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
        let localStream;

        // Teachable Machine Model URL
        const modelURL = "https://deanaxelrod.github.io/Google-TM/model.json";

        // Load Teachable Machine model
        let model;
        async function loadModel() {
            model = await tf.loadLayersModel(modelURL);
            console.log("Model loaded");
        }

        // Function to classify audio
        async function classifyAudio(audioBlob) {
            const audioContext = new AudioContext();
            const arrayBuffer = await audioBlob.arrayBuffer();
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

            // Convert audio to spectrogram (example, adjust as needed)
            const spectrogram = convertToSpectrogram(audioBuffer);
            const prediction = await model.predict(spectrogram);
            const result = prediction.argMax(1).dataSync()[0];
            return result;
        }

        // Function to get OpenAI prompt
        async function getOpenAIPrompt(classification) {
            const response = await fetch("https://api.openai.com/v1/chat/completions", {
                method: "POST",
                headers: {
                    "Authorization": "Bearer sk-proj-Og4Mrv9E-W8W_cu2VFaKEiyuV8ZfvsblUI0s3XiX91HYJzh3CGsWTgO4-MtXvpAUk-Y96g8Fr1T3BlbkFJV-WEta2aJyxYIiFUcGkYWfmP6xNZandvivEPMMAVu0PFGoJq2tS8Gn01J0Z07sA0PZ9a6kMbgA",
                    "Content-Type": "application/json"
                },
                body: JSON.stringify({
                    model: "gpt-3.5-turbo",
                    messages: [
                        {
                            role: "user",
                            content: `Translate this pet sound classification into a human-like sentence: ${classification}`
                        }
                    ]
                })
            });
            const data = await response.json();
            return data.choices[0].message.content;
        }

        // Function to send status to Thunkable
        function sendStatus(status) {
            if (window.ReactNativeWebView) {
                window.ReactNativeWebView.postMessage(status);
            }
        }

        // Initialize Agora
        client.init(appId, () => {
            console.log("Agora initialized");
            document.getElementById("status").innerText = "Status: Ready";
            loadModel();
        });

        // Start Recording
        document.getElementById("startBtn").onclick = async () => {
            try {
                const channelName = "test_channel";
                const uid = await client.join(null, channelName, null);
                console.log("User joined with UID:", uid);

                localStream = AgoraRTC.createStream({ audio: true, video: false });
                await localStream.init();
                client.publish(localStream);
                console.log("Recording started");

                document.getElementById("status").innerText = "Status: Recording";
                document.getElementById("startBtn").disabled = true;
                document.getElementById("stopBtn").disabled = false;
                sendStatus("Recording started");
            } catch (error) {
                console.error("Error starting recording:", error);
                sendStatus("Error: " + error.message);
            }
        };

        // Stop Recording
        document.getElementById("stopBtn").onclick = async () => {
            try {
                client.unpublish(localStream);
                await client.leave();
                localStream.close();
                console.log("Recording stopped");

                // Classify the audio using Teachable Machine
                const audioBlob = await localStream.getAudioTrack().getBlob();
                const result = await classifyAudio(audioBlob);
                sendStatus("Classification: " + result);

                // Get OpenAI prompt
                const prompt = await getOpenAIPrompt(result);
                sendStatus("OpenAI Prompt: " + prompt);

                document.getElementById("status").innerText = "Status: Stopped";
                document.getElementById("startBtn").disabled = false;
                document.getElementById("stopBtn").disabled = true;
            } catch (error) {
                console.error("Error stopping recording:", error);
                sendStatus("Error: " + error.message);
            }
        };
    </script>
</body>
</html>
