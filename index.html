<!DOCTYPE html>
<html>
<head>
  <title>Agora Voice Recorder with Classification</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands"></script>
  <script src="https://cdn.agora.io/sdk/release/AgoraRTCSDK-3.6.9.js"></script>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      margin: 0; 
      padding: 20px; 
      box-sizing: border-box; 
    }
    .container { 
      max-width: 600px; 
      margin: 0 auto; 
    }
    .button-container { 
      margin: 20px 0; 
      display: flex; 
      justify-content: center; 
      gap: 10px; 
    }
    button {
      margin: 5px; 
      padding: 15px 30px; 
      cursor: pointer;
      background-color: #4CAF50; 
      color: white; 
      border: none;
      border-radius: 4px; 
      font-size: 16px; 
      min-width: 200px;
      transition: all 0.3s ease;
    }
    button:disabled { 
      background-color: #cccccc; 
      cursor: not-allowed; 
    }
    .stop-button { 
      background-color: #cccccc; 
    }
    .stop-button.recording { 
      background-color: #f44336 !important; 
    }
    #debug { 
      margin-top: 20px; 
      padding: 10px; 
      border: 1px solid #ccc; 
      max-height: 300px; 
      overflow-y: auto; 
      font-size: 12px; 
    }
    .status { 
      font-size: 18px; 
      margin: 10px 0; 
      text-align: center; 
    }
    #permissionMessage {
      color: red;
      text-align: center;
      margin-bottom: 20px;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Agora Voice Recorder with Classification</h1>
    <div id="permissionMessage"></div>
    <div class="button-container">
      <button id="startBtn">Start Recording</button>
      <button id="stopBtn" class="stop-button" disabled>Stop Recording</button>
    </div>
    <p id="recordingStatus" class="status">Recording Status: Ready</p>
    <p id="classificationResult" class="status">Classification Result: N/A</p>
    <div id="debug"></div>
  </div>

  <script>
    const appId = "25336bcd01664c70870804ba0b7e1b30";
    const modelURL = "https://deanaxelrod.github.io/Google-TM/model.json";
    const classLabels = ["Noise", "Sad Dog", "Excited Dog", "Disappointed Dog", "Growl", "Whimper", "Bark"];

    let mediaRecorder = null;
    let recordedChunks = [];
    let model = null;

    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const statusEl = document.getElementById('recordingStatus');
    const resultEl = document.getElementById('classificationResult');
    const permissionMsgEl = document.getElementById('permissionMessage');

    function debugLog(message) {
      const debugDiv = document.getElementById('debug');
      const logEntry = document.createElement('p');
      logEntry.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
      debugDiv.appendChild(logEntry);
      debugDiv.scrollTop = debugDiv.scrollHeight;
      console.log(message); // Also log to console for additional debugging
    }

    function setRecordingState(recording) {
      startBtn.disabled = recording;
      stopBtn.disabled = !recording;
      statusEl.textContent = `Recording Status: ${recording ? 'Recording' : 'Ready'}`;

      if (recording) {
        stopBtn.classList.add('recording');
      } else {
        stopBtn.classList.remove('recording');
      }
    }

    async function checkMicrophonePermission() {
      try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
          throw new Error('getUserMedia not supported');
        }

        // Try to query permissions if supported
        if (navigator.permissions) {
          const permissionStatus = await navigator.permissions.query({ name: 'microphone' });
          debugLog(`Microphone Permission Status: ${permissionStatus.state}`);

          permissionStatus.onchange = () => {
            debugLog(`Microphone permission changed to: ${permissionStatus.state}`);
          };

          if (permissionStatus.state === 'denied') {
            permissionMsgEl.textContent = 'Microphone access is blocked. Please enable in device settings.';
            throw new Error('Microphone access denied');
          }
        }

        // Attempt to get user media to verify real-time access
        await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });

        permissionMsgEl.textContent = ''; // Clear any previous error messages
        return true;
      } catch (error) {
        debugLog(`Microphone Permission Error: ${error.message}`);
        permissionMsgEl.textContent = `Microphone Access Error: ${error.message}. Please check browser settings.`;
        startBtn.disabled = true;
        return false;
      }
    }

    async function createSpectrogram(audioBuffer) {
      try {
        debugLog("Creating spectrogram...");

        const audioData = audioBuffer.getChannelData(0);
        const fftSize = 512;
        const hopSize = fftSize / 2;
        const numFrames = Math.floor((audioData.length - fftSize) / hopSize);

        const spectrogramData = [];

        for (let i = 0; i < numFrames; i++) {
          const startIdx = i * hopSize;
          const frameData = audioData.slice(startIdx, startIdx + fftSize);

          if (frameData.length === fftSize) {
            const tensorFrame = tf.tensor1d(frameData);
            const windowedFrame = tensorFrame.mul(tf.signal.hammingWindow(fftSize));
            const fft = tf.spectral.rfft(windowedFrame);
            const magnitude = tf.abs(fft);

            spectrogramData.push(magnitude.arraySync());
            fft.dispose();
            tensorFrame.dispose();
          }
        }

        const spectrogramTensor = tf.tensor2d(spectrogramData);
        debugLog(`Spectrogram created with shape: ${spectrogramTensor.shape}`);

        const targetHeight = 43;
        const targetWidth = 232;

        const spectrogramWithChannel = spectrogramTensor.reshape([1, spectrogramTensor.shape[0], spectrogramTensor.shape[1], 1]);
        const resizedSpectrogram = tf.image.resizeBilinear(spectrogramWithChannel, [targetHeight, targetWidth]);
        spectrogramTensor.dispose();

        const normalizedSpectrogram = resizedSpectrogram.div(tf.max(resizedSpectrogram));
        return normalizedSpectrogram.reshape([1, targetHeight, targetWidth, 1]);
      } catch (error) {
        debugLog("Error creating spectrogram: " + error.message);
        throw error;
      }
    }

async function classifyAudio(audioBlob) {
  try {
    debugLog("Starting classification...");
    const audioContext = new AudioContext();
    const arrayBuffer = await audioBlob.arrayBuffer();

    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    debugLog("Audio decoded, duration: " + audioBuffer.duration);

    const spectrogram = await createSpectrogram(audioBuffer);
    const prediction = await model.predict(spectrogram).data();
    spectrogram.dispose();

    classLabels.forEach((label, idx) => {
      debugLog(`${label}: ${(prediction[idx] * 100).toFixed(2)}%`);
    });

    const resultIndex = prediction.indexOf(Math.max(...prediction));
    const classificationLabel = classLabels[resultIndex];

    debugLog("About to send message to parent");
    const message = {
      data: {  // Add a data wrapper
        type: 'audioClassification',
        source: 'recorder',
        classification: classificationLabel
      }
    };
    debugLog("Sending message: " + JSON.stringify(message));
    
    window.parent.postMessage(message, '*');
    console.log('Message sent:', classificationLabel);
    debugLog("Message sent");

    return classificationLabel;
  } catch (error) {
    debugLog("Error: " + error.message);
  }
}

    async function startRecording() {
      try {
        // Ensure microphone permission is granted
        const hasPermission = await checkMicrophonePermission();
        if (!hasPermission) {
          debugLog("Microphone permission not granted");
          return;
        }

        debugLog("Starting recording...");
        setRecordingState(true);

        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });

        mediaRecorder = new MediaRecorder(stream);
        recordedChunks = [];

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            recordedChunks.push(event.data);
          }
        };

        mediaRecorder.onstart = () => {
          debugLog("MediaRecorder started");
        };

        mediaRecorder.start();
      } catch (error) {
        debugLog("Error starting recording: " + error.message);
        permissionMsgEl.textContent = `Recording Error: ${error.message}`;
        setRecordingState(false);
      }
    }

    async function stopRecording() {
      try {
        debugLog("Stopping recording...");
        statusEl.textContent = "Recording Status: Processing...";

        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
          await new Promise(resolve => {
            mediaRecorder.onstop = resolve;
            mediaRecorder.stop();
          });
          debugLog("MediaRecorder stopped");
        }

        if (recordedChunks.length > 0) {
          const audioBlob = new Blob(recordedChunks, { type: 'audio/webm' });
          debugLog("Processing audio...");

          try {
            const classificationLabel = await classifyAudio(audioBlob);
            resultEl.textContent = "Classification Result: " + classificationLabel;
            
            // Send classification via postMessage with specific data
            window.parent.postMessage({
              type: 'audioClassification',
              classification: classificationLabel
           }, '*');
            
            debugLog("Classification complete: " + classificationLabel);
          } catch (error) {
            debugLog("Classification failed: " + error.message);
            resultEl.textContent = "Classification Result: Error";
            window.parent.postMessage({
              type: 'audioClassificationError',
              error: error.message
            }, '*');
          }
        }

        setRecordingState(false);
      } catch (error) {
        debugLog("Error stopping recording: " + error.message);
        setRecordingState(false);
      }
    }

async function initialize() {
  try {
    debugLog("Loading model...");
    await tf.setBackend('cpu');
    model = await tf.loadLayersModel(modelURL);
    debugLog("Model loaded successfully");
    startBtn.disabled = false;
    statusEl.textContent = "Recording Status: Ready";
  } catch (error) {
    debugLog("Initialization error: " + error.message);
    startBtn.disabled = true;
    statusEl.textContent = "Recording Initialization Failed";
    permissionMsgEl.textContent = `Initialization Error: ${error.message}`;
  }
}

    // Event Listeners
    startBtn.addEventListener('click', async () => {
      try {
        // Ensure this is directly triggered by user interaction
        await checkMicrophonePermission();
        await startRecording();
      } catch (error) {
        debugLog("Recording start error: " + error.message);
      }
    });

    stopBtn.addEventListener('click', stopRecording);

    window.addEventListener('beforeunload', () => {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }
    });

    // Initialize on page load
    initialize();
  </script>
</body>
</html>
